{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vos8ORFrA-0P"
      },
      "outputs": [],
      "source": [
        "! pip install -Uq torch torchvision tensorboard sentence-transformers datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv6KE-G6V6LZ"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hel_yxd0BXug"
      },
      "outputs": [],
      "source": [
        "from datasets.utils.logging import disable_progress_bar\n",
        "disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jlYwO8mBWI4"
      },
      "source": [
        "#### **Create and Prepare embedding dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAZEttOuBNx4",
        "outputId": "46d14417-3532-4f13-afa6-2a920101f965"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:86: UserWarning: \n",
            "Access to the secret `HF_TOKEN` has not been granted on this notebook.\n",
            "You will not be requested again.\n",
            "Please restart the session if you want to be prompted again.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['query_id', 'passage_id', 'query', 'passage', 'category', 'link'],\n",
              "    num_rows: 44708\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rasyosef/amharic-passage-retrieval-dataset\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO6ZPH-1rh_7"
      },
      "outputs": [],
      "source": [
        "# rename columns\n",
        "dataset = dataset.rename_column(\"query\", \"anchor\")\n",
        "dataset = dataset.rename_column(\"passage\", \"positive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54E8BEoCCN6_",
        "outputId": "5c744d47-2d50-469b-e848-1bb4abe1afa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "    num_rows: 44708\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add an id column to the dataset\n",
        "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9JR85l2CbWp",
        "outputId": "1842acf1-e3fb-45f1-f660-a04dc24b7a91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "        num_rows: 40237\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "        num_rows: 4471\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split dataset into a 10% test set\n",
        "dataset = dataset.class_encode_column(\"category\")\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=16, stratify_by_column=\"category\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GoNKHRrDN3p"
      },
      "source": [
        "#### **Create baseline and evaluate pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqOtozrgDKvH",
        "outputId": "0eaecd8b-9c46-4eb7-856e-8301a16f9cc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "    num_rows: 44708\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "corpus_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pdE7K4jHSky"
      },
      "outputs": [],
      "source": [
        "# Convert the datasets to dictionaries\n",
        "corpus = dict(\n",
        "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
        ") # Our corpus (cid => document)\n",
        "queries = dict(\n",
        "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
        ") # Our queries (qid => question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfj09jsPIsns"
      },
      "outputs": [],
      "source": [
        "# Create a mapping of relevant document (1 in our case) for each query\n",
        "relevant_docs = {}\n",
        "for q_id in queries:\n",
        "  relevant_docs[q_id] = [q_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei9-g0UmjLkA"
      },
      "source": [
        "#### **Evaluate RoBERTa Base Amharic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuqcMD9nigo5",
        "outputId": "1b6de412-59fd-4dee-d45d-f3c17b78df3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at rasyosef/bert-medium-amharic and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, SentenceTransformerModelCardData\n",
        "from sentence_transformers.models import Transformer, Pooling, Normalize\n",
        "\n",
        "base_model = \"rasyosef/bert-medium-amharic\"\n",
        "\n",
        "model = SentenceTransformer(\n",
        "    modules=[\n",
        "      Transformer(**{\"model_name_or_path\":base_model, \"tokenizer_name_or_path\":base_model}),\n",
        "      Pooling(**{'word_embedding_dimension': 512, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True}),\n",
        "      Normalize()\n",
        "    ],\n",
        "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
        "    model_card_data=SentenceTransformerModelCardData(\n",
        "        language=\"en\",\n",
        "        license=\"apache-2.0\",\n",
        "        model_name=\"BERT Amharic Text Embedding Medium\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCZdGcu1J6NQ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator, SequentialEvaluator\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "EMBED_DIM = model.get_sentence_embedding_dimension()\n",
        "matryoshka_dimensions = [EMBED_DIM, 256]\n",
        "\n",
        "matryoshka_evaluators = []\n",
        "# Iterate over the different dimensions\n",
        "for dim in matryoshka_dimensions:\n",
        "  ir_evaluator = InformationRetrievalEvaluator(\n",
        "      queries=queries,\n",
        "      corpus=corpus,\n",
        "      relevant_docs=relevant_docs,\n",
        "      name=f\"dim_{dim}\",\n",
        "      truncate_dim=dim,\n",
        "      score_functions={\"cosine\": cos_sim},\n",
        "      batch_size=128,\n",
        "      corpus_chunk_size=2048,\n",
        "      show_progress_bar=False\n",
        "  )\n",
        "  matryoshka_evaluators.append(ir_evaluator)\n",
        "\n",
        "# Create a sequential evaluator\n",
        "evaluator = SequentialEvaluator(matryoshka_evaluators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEF7LpCwMxnn",
        "outputId": "f143afd3-7311-43bb-e1ff-dab734cfeb40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dim_512_cosine_ndcg@10: 0.1552041865501883\n",
            "dim_256_cosine_ndcg@10: 0.11780929529557317\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "results = evaluator(model)\n",
        "\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_ndcg@10\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUtaXsBjXEG5",
        "outputId": "5c42c31d-4283-4006-caf7-8bb68befcf4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dim_512_cosine_recall@5: 0.17960187877432343\n",
            "dim_256_cosine_recall@5: 0.14068441064638784\n"
          ]
        }
      ],
      "source": [
        "# print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_recall@5\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK2AgA-XOzN1"
      },
      "source": [
        "#### **Define loss function with Matryoshka Representation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkU2KuBbQHqE"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [EMBED_DIM, 256]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st9kVcsLQ5A2"
      },
      "source": [
        "#### **Fine-tune embedding model with** `SentenceTransformersTrainer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oisTjY5wQtue"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"bert-medium-amharic-embedding-matryoshka\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=128,\n",
        "    # gradient_accumulation_steps=2,\n",
        "    per_device_eval_batch_size=128,\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    fp16=True,\n",
        "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=None,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_dim_256_cosine_ndcg@10\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exCaRnlxTQDI"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerTrainer\n",
        "\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args, # training arguments\n",
        "    train_dataset=train_dataset.select_columns(\n",
        "        [\"anchor\", \"positive\"]\n",
        "    ), # training dataset\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHsybW4kTzjq"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk19N8f0UYdm"
      },
      "outputs": [],
      "source": [
        "# save the best model\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KScLm3RGY0HF"
      },
      "source": [
        "#### **Evaluate fine-tuned model against baseline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX0iVR8WYxcV",
        "outputId": "c5c3ba65-c873-4fbd-dacd-8d6193b72b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dim_512_cosine_ndcg@10: 0.7205067854426142\n",
            "dim_256_cosine_ndcg@10: 0.7137937431915936\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "fine_tuned_model = SentenceTransformer(\n",
        "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "results = evaluator(fine_tuned_model)\n",
        "\n",
        "# print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_ndcg@10\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzUMk3O4W58E",
        "outputId": "1d0a7dde-7372-41a2-87ff-413b17efa798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dim_512_cosine_recall@5: 0.7877432341757996\n",
            "dim_256_cosine_recall@5: 0.7846119436367703\n"
          ]
        }
      ],
      "source": [
        "# print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_recall@5\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etv93oAUCg54"
      },
      "source": [
        "### **Push Model to HuggingFace**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu3emzlmZ6v0",
        "outputId": "943767d0-d68f-428f-fbdc-c3868e28085c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 512, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X92sbGe9jqF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_WRITE_TOKEN\")\n",
        "\n",
        "# push model to hub\n",
        "trainer.model.push_to_hub(\"bert-amharic-embed-medium-45k-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iby6X7XMCnvP"
      },
      "source": [
        "### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt8j2nlO4Hyk",
        "outputId": "4fa197b8-4c16-4dd8-9c5c-bfd7da761a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5, 512)\n",
            "tensor([[ 1.0000, -0.0483,  0.0084, -0.0344,  0.5911],\n",
            "        [-0.0483,  1.0000,  0.4262,  0.2786,  0.0830],\n",
            "        [ 0.0084,  0.4262,  1.0000,  0.2788,  0.2136],\n",
            "        [-0.0344,  0.2786,  0.2788,  1.0000,  0.0721],\n",
            "        [ 0.5911,  0.0830,  0.2136,  0.0721,  1.0000]])\n"
          ]
        }
      ],
      "source": [
        "# The sentences to encode\n",
        "sentences = [\n",
        "  \"የተደጋገመው የመሬት መንቀጥቀጥና የእሳተ ገሞራ ምልክት በአፋር ክልል\",\n",
        "  \"በአክሱም ከተማ የሚገኙ ሙስሊም ሴት ተማሪዎች ከሒጃብ መልበስ ጋር በተያያዘ ውዝግብ ከትምህርት ገበታ ውጭ ሆነው እንደሚገኙ የትግራይ እስልምና ጉዳዮች ምክርቤት ስታወቀ። ይህን ለመፍታት ከክልሉ ትምህርት ቢሮ ጋር ንግግር ላይ መሆኑም የክልሉ እስልምና ጉዳዮች ምክርቤት ለዶቼቬለ ገልጿል።\",\n",
        "  \"በማዕከላዊ ኢትዮጵያ ክልል ሃድያ ዞን ጊቤ ወረዳ በሚገኙ 12 ቀበሌዎች መሠረታዊ የመንግሥት አገልግሎት መስጫ ተቋማት በሙሉና በከፊል በመዘጋታቸው መቸገራቸውን ነዋሪዎች አመለከቱ። ከባለፈው ዓመት ጀምሮ የጤና፣ የትምህርት እና የግብር አሰባሰብ ሥራዎች በየአካባቢያቸው እየተከናወኑ አለመሆናቸውንም ለዶቼ ቬለ ተናግረዋል።\",\n",
        "  \"የሕዝብ ተወካዮች ምክር ቤት አባል እና የቋሚ ኮሚቴ ሰብሳቢ የነበሩት አቶ ክርስቲያን ታደለ እና የአማራ ክልል ምክር ቤት አባል የሆኑት አቶ ዮሐንስ ቧያለው ከቃሊቲ ወደ ቂሊንጦ ማረሚያ ቤት መዛወራቸውን ጠበቃቸው ተናገሩ።\",\n",
        "  \"ከተደጋጋሚ መሬት መንቀጥቀጥ በኋላ አፋር ክልል እሳት ከመሬት ውስጥ ሲፈላ ታይቷል፡፡ ከመሬት ውስጥ እሳትና ጭስ የሚተፋው እንፋሎቱ ዛሬ ማለዳውን 11 ሰዓት ግድም ከከባድ ፍንዳታ በኋላየተስተዋለ መሆኑን የአከባቢው ነዋሪዎች እና ባለስልጣናት ለዶቼ ቬለ ተናግረዋል፡፡ አለት የሚያፈናጥር እሳት ነው የተባለው እንፋሎቱ በክልሉ ጋቢረሱ (ዞን 03) ዱለቻ ወረዳ ሰጋንቶ ቀበሌ መከሰቱን የገለጹት የአከባቢው የአይን እማኞች ከዋናው ፍንዳታ በተጨማሪ በዙሪያው ተጨማሪ ፍንዳታዎች መታየት ቀጥሏል ባይ ናቸው፡፡\"\n",
        "]\n",
        "\n",
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings.shape)\n",
        "# [3, 384]\n",
        "\n",
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyjAhXbLqDOx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
