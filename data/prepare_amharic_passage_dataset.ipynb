{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49971 documents.\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "INPUT_FILE = \"./dataset/raw/amharic-news_dataset/amharic_news_classification_dataset.jsonl\"\n",
    "HOME_DIR = \"./dataset/processed/msmarco-amharic-news_dataset\"\n",
    "os.makedirs(HOME_DIR, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(input_file):\n",
    "    data = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded {len(df)} documents.\")\n",
    "    return df\n",
    "\n",
    "df = load_dataset(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensure required columns exist\n",
    "assert \"article\" in df.columns, \"Missing column: 'article'\"\n",
    "assert \"headline\" in df.columns, \"Missing column: 'headline'\"\n",
    "assert \"label\" in df.columns, \"Missing column: 'label'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(df.isnull().sum())\n",
    "print(df[\"headline\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean articles\n",
    "df[\"article\"] = df[\"article\"].astype(str).str.strip().replace(r\"\\s+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates. Remaining documents: 49839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to generate MD5 hash\n",
    "def generate_md5(text):\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Apply MD5 hashing to remove duplicates\n",
    "df[\"hash\"] = df[\"article\"].apply(generate_md5)\n",
    "df = df.drop_duplicates(subset=\"hash\", keep=\"first\").drop(columns=[\"hash\"])  # Drop duplicate articles\n",
    "\n",
    "print(f\"Removed duplicates. Remaining documents: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure labels appear at least twice to avoid stratification issues\n",
    "df = df.groupby(\"label\").filter(lambda x: len(x) > 1)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique passages: 49839\n",
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label', 'passage_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assign unique passage and query IDs\n",
    "df[\"passage_id\"] = range(len(df))\n",
    "passage_dict = dict(zip(df[\"article\"], df[\"passage_id\"]))\n",
    "print(f\"Number of unique passages: {df['passage_id'].nunique()}\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      headline category              date  \\\n",
      "0  የኦሊምፒክ ማጣሪያ ተሳታፊዎች የሚለዩበት ቻምፒዮና እየተካሄደ ይገኛል     ስፖርት  January 14, 2021   \n",
      "\n",
      "  views                                            article  \\\n",
      "0     2  ብርሃን ፈይሳየኢትዮጵያ ቦክስ ፌዴሬሽን በየዓመቱ የሚያዘጋጀው የክለቦች ቻ...   \n",
      "\n",
      "                                link  word_len  label  passage_id  \n",
      "0  https://www.press.et/Ama/?p=39481       298      2           0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique passages:  49839\n",
      "Collection saved!,(49839 passages)\n"
     ]
    }
   ],
   "source": [
    "# Save Passage Collection\n",
    "collection_jsonl = os.path.join(HOME_DIR, \"collection.jsonl\")\n",
    "collection_tsv = os.path.join(HOME_DIR, \"collection.tsv\")\n",
    "i=0\n",
    "\n",
    "with open(collection_jsonl, \"w\", encoding=\"utf-8\") as f_jsonl, open(collection_tsv, \"w\", encoding=\"utf-8\") as f_tsv:\n",
    "    for _, row in df.iterrows():\n",
    "        json.dump({\"pid\": row[\"passage_id\"], \"text\": row[\"article\"]}, f_jsonl, ensure_ascii=False)\n",
    "        f_jsonl.write(\"\\n\")\n",
    "        f_tsv.write(f\"{row['passage_id']}\\t{row['article']}\\n\")\n",
    "        i+=1\n",
    "\n",
    "print(\"number of unique passages: \", i)\n",
    "print(f\"Collection saved!,({len(df)} passages)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    20324\n",
      "2    10104\n",
      "5     9129\n",
      "4     5849\n",
      "3     3802\n",
      "1      631\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 44855 | Dev: 4984\n"
     ]
    }
   ],
   "source": [
    "# Split into Train & Dev\n",
    "train_df, dev_df = train_test_split(df, test_size=0.1, stratify=df[\"label\"], random_state=42)\n",
    "print(f\"Train: {len(train_df)} | Dev: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label', 'passage_id', 'query_id'],\n",
      "      dtype='object')\n",
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label', 'passage_id', 'query_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.copy().reset_index(drop=True)\n",
    "dev_df = dev_df.copy().reset_index(drop=True)\n",
    "\n",
    "train_df[\"query_id\"] = range(len(train_df))  # Assign unique IDs for train\n",
    "dev_df[\"query_id\"] = range(len(train_df), len(train_df) + len(dev_df))  # Continue IDs for dev\n",
    "print(train_df.columns)\n",
    "print(dev_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    18291\n",
      "2     9094\n",
      "5     8216\n",
      "4     5264\n",
      "3     3422\n",
      "1      568\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    2033\n",
      "2    1010\n",
      "5     913\n",
      "4     585\n",
      "3     380\n",
      "1      63\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"label\"].value_counts())\n",
    "print(dev_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Distribution:\n",
      " label\n",
      "0    0.407781\n",
      "2    0.202742\n",
      "5    0.183168\n",
      "4    0.117356\n",
      "3    0.076290\n",
      "1    0.012663\n",
      "Name: proportion, dtype: float64\n",
      "Dev Label Distribution:\n",
      " label\n",
      "0    0.407905\n",
      "2    0.202648\n",
      "5    0.183186\n",
      "4    0.117376\n",
      "3    0.076244\n",
      "1    0.012640\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Label Distribution:\\n\", train_df[\"label\"].value_counts(normalize=True))\n",
    "print(\"Dev Label Distribution:\\n\", dev_df[\"label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"UNKNOWN_QUERY\"\n",
    "    return text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_queries_and_qrels(dataframe, split_name):\n",
    "    queries_jsonl = os.path.join(HOME_DIR, f\"queries_{split_name}.jsonl\")\n",
    "    queries_tsv = os.path.join(HOME_DIR, f\"queries_{split_name}.tsv\")\n",
    "    qrels_tsv = os.path.join(HOME_DIR, f\"qrels_{split_name}.tsv\")\n",
    "    \n",
    "    with open(queries_jsonl, \"w\", encoding=\"utf-8\") as f_jsonl, open(queries_tsv, \"w\", encoding=\"utf-8\") as f_tsv, open(qrels_tsv, \"w\", encoding=\"utf-8\") as f_qrels:\n",
    "        for _, row in dataframe.iterrows():\n",
    "            cleaned_query = clean_query(row[\"headline\"])\n",
    "            json.dump({\"query_id\": row[\"query_id\"], \"headline\": cleaned_query}, f_jsonl, ensure_ascii=False)\n",
    "            f_jsonl.write(\"\\n\")\n",
    "            f_tsv.write(f\"{row['query_id']}\\t{cleaned_query}\\n\")\n",
    "            f_qrels.write(f\"{row['query_id']}\\t0\\t{row['passage_id']}\\t1\\n\")\n",
    "\n",
    "    print(f\"Saved {split_name} queries and qrels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate queries or passages found!\n"
     ]
    }
   ],
   "source": [
    "assert train_df[\"query_id\"].nunique() == len(train_df), \" Duplicate queries in train set!\"\n",
    "assert dev_df[\"query_id\"].nunique() == len(dev_df), \" Duplicate queries in dev set!\"\n",
    "assert df[\"passage_id\"].nunique() == len(df), \" Duplicate passages in dataset!\"\n",
    "\n",
    "print(\"No duplicate queries or passages found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train queries and qrels.\n",
      "Saved dev queries and qrels.\n"
     ]
    }
   ],
   "source": [
    "#Save train and dev datasets\n",
    "save_queries_and_qrels(train_df, \"train\")\n",
    "save_queries_and_qrels(dev_df, \"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amharic_colbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
