{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vos8ORFrA-0P"
      },
      "outputs": [],
      "source": [
        "! pip install -Uq torch torchvision tensorboard sentence-transformers datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")"
      ],
      "metadata": {
        "id": "Tv6KE-G6V6LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Create and Prepare embedding dataset**"
      ],
      "metadata": {
        "id": "_jlYwO8mBWI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rasyosef/amharic-news-retrieval-dataset\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAZEttOuBNx4",
        "outputId": "46d7f335-5f06-46ef-9f91-f5374c22e76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['query_id', 'passage_id', 'query', 'passage', 'category', 'link'],\n",
              "    num_rows: 44708\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rename columns\n",
        "dataset = dataset.rename_column(\"query\", \"anchor\")\n",
        "dataset = dataset.rename_column(\"passage\", \"positive\")"
      ],
      "metadata": {
        "id": "IO6ZPH-1rh_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an id column to the dataset\n",
        "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl2lNJ--syE5",
        "outputId": "f14ec20e-d517-4fde-8bb9-05580020fa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "    num_rows: 44708\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into a 10% test set\n",
        "dataset = dataset.class_encode_column(\"category\")\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=16, stratify_by_column=\"category\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5K8B2NStVc2",
        "outputId": "07981faa-dd26-4e67-d228-9b289a95e7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "        num_rows: 40237\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "        num_rows: 4471\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Create baseline and evaluate pretrained model**"
      ],
      "metadata": {
        "id": "4GoNKHRrDN3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "corpus_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqOtozrgDKvH",
        "outputId": "6e47dea9-c5e0-4ed7-fc63-a800a3899cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['query_id', 'passage_id', 'anchor', 'positive', 'category', 'link', 'id'],\n",
              "    num_rows: 44708\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the datasets to dictionaries\n",
        "corpus = dict(\n",
        "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
        ") # Our corpus (cid => document)\n",
        "queries = dict(\n",
        "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
        ") # Our queries (qid => question)"
      ],
      "metadata": {
        "id": "3pdE7K4jHSky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping of relevant document (1 in our case) for each query\n",
        "relevant_docs = {}\n",
        "for q_id in queries:\n",
        "  relevant_docs[q_id] = [q_id]"
      ],
      "metadata": {
        "id": "Pfj09jsPIsns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Initialize Embedding model**"
      ],
      "metadata": {
        "id": "ei9-g0UmjLkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, SentenceTransformerModelCardData\n",
        "from sentence_transformers.models import Transformer, Pooling, Normalize\n",
        "\n",
        "base_model = \"rasyosef/roberta-medium-amharic\"\n",
        "\n",
        "model = SentenceTransformer(\n",
        "    modules=[\n",
        "      Transformer(**{\"model_name_or_path\":base_model, \"tokenizer_name_or_path\":base_model}),\n",
        "      Pooling(**{'word_embedding_dimension': 512, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True}),\n",
        "      Normalize()\n",
        "    ],\n",
        "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
        "    model_card_data=SentenceTransformerModelCardData(\n",
        "        language=\"en\",\n",
        "        license=\"apache-2.0\",\n",
        "        model_name=\"RoBERTa Amharic Text Embedding Medium\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "dogaBoiSNYB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a779b84e-dc76-47dc-f622-85f1c27b821e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at rasyosef/roberta-medium-amharic and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator, SequentialEvaluator\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "EMBED_DIM = model.get_sentence_embedding_dimension()\n",
        "matryoshka_dimensions = [EMBED_DIM, 256]\n",
        "\n",
        "matryoshka_evaluators = []\n",
        "# Iterate over the different dimensions\n",
        "for dim in matryoshka_dimensions:\n",
        "  ir_evaluator = InformationRetrievalEvaluator(\n",
        "      queries=queries,\n",
        "      corpus=corpus,\n",
        "      relevant_docs=relevant_docs,\n",
        "      name=f\"dim_{dim}\",\n",
        "      truncate_dim=dim,\n",
        "      score_functions={\"cosine\": cos_sim},\n",
        "      batch_size=128,\n",
        "      corpus_chunk_size=2048,\n",
        "      show_progress_bar=False\n",
        "  )\n",
        "  matryoshka_evaluators.append(ir_evaluator)\n",
        "\n",
        "# Create a sequential evaluator\n",
        "evaluator = SequentialEvaluator(matryoshka_evaluators)"
      ],
      "metadata": {
        "id": "qCZdGcu1J6NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = evaluator(model)\n",
        "\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_ndcg@10\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ],
      "metadata": {
        "id": "JEF7LpCwMxnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a911a1-f38b-4191-d28a-36f64df1bf1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim_512_cosine_ndcg@10: 0.08174514226156712\n",
            "dim_256_cosine_ndcg@10: 0.06503403305545333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_recall@5\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ],
      "metadata": {
        "id": "tUtaXsBjXEG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865d3b0e-5b03-4982-b73e-90af335816dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim_512_cosine_recall@5: 0.09438604339074033\n",
            "dim_256_cosine_recall@5: 0.07783493625587116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define loss function with Matryoshka Representation**"
      ],
      "metadata": {
        "id": "RK2AgA-XOzN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [EMBED_DIM, 256]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ],
      "metadata": {
        "id": "RkU2KuBbQHqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Fine-tune embedding model with** `SentenceTransformersTrainer`"
      ],
      "metadata": {
        "id": "st9kVcsLQ5A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"roberta-medium-amharic-embedding-matryoshka\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=128,\n",
        "    # gradient_accumulation_steps=2,\n",
        "    per_device_eval_batch_size=128,\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    fp16=True,\n",
        "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=None,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_dim_256_cosine_ndcg@10\",\n",
        ")"
      ],
      "metadata": {
        "id": "oisTjY5wQtue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformerTrainer\n",
        "\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args, # training arguments\n",
        "    train_dataset=train_dataset.select_columns(\n",
        "        [\"anchor\", \"positive\"]\n",
        "    ), # training dataset\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")"
      ],
      "metadata": {
        "id": "exCaRnlxTQDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "HHsybW4kTzjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the best model\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "yk19N8f0UYdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Evaluate fine-tuned model against baseline**"
      ],
      "metadata": {
        "id": "KScLm3RGY0HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "fine_tuned_model = SentenceTransformer(\n",
        "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "results = evaluator(fine_tuned_model)\n",
        "\n",
        "# print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_ndcg@10\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX0iVR8WYxcV",
        "outputId": "155b1553-c84b-4dc4-c7df-bcd99afdeef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim_512_cosine_ndcg@10: 0.7713405401987244\n",
            "dim_256_cosine_ndcg@10: 0.7664475358787538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "  key = f\"dim_{dim}_cosine_recall@5\"\n",
        "  print(f\"{key}: {results[key]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzUMk3O4W58E",
        "outputId": "ca0acfe0-9839-478b-b452-a5e4e7aa42b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim_512_cosine_recall@5: 0.8378438828002684\n",
            "dim_256_cosine_recall@5: 0.8356072467009618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Push Model to HuggingFace**"
      ],
      "metadata": {
        "id": "Pnmw-mA3QFGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu3emzlmZ6v0",
        "outputId": "edd4e347-96e2-4bcb-bc00-983e015b9447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 510, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
              "  (1): Pooling({'word_embedding_dimension': 512, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_WRITE_TOKEN\")\n",
        "\n",
        "# push model to hub\n",
        "trainer.model.push_to_hub(\"roberta-amharic-embed-medium-45k\")"
      ],
      "metadata": {
        "id": "9X92sbGe9jqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example**"
      ],
      "metadata": {
        "id": "MvOfklq2P0bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The sentences to encode\n",
        "sentences = [\n",
        "  \"የተደጋገመው የመሬት መንቀጥቀጥና የእሳተ ገሞራ ምልክት በአፋር ክልል\",\n",
        "  \"በአክሱም ከተማ የሚገኙ ሙስሊም ሴት ተማሪዎች ከሒጃብ መልበስ ጋር በተያያዘ ውዝግብ ከትምህርት ገበታ ውጭ ሆነው እንደሚገኙ የትግራይ እስልምና ጉዳዮች ምክርቤት ስታወቀ። ይህን ለመፍታት ከክልሉ ትምህርት ቢሮ ጋር ንግግር ላይ መሆኑም የክልሉ እስልምና ጉዳዮች ምክርቤት ለዶቼቬለ ገልጿል።\",\n",
        "  \"በማዕከላዊ ኢትዮጵያ ክልል ሃድያ ዞን ጊቤ ወረዳ በሚገኙ 12 ቀበሌዎች መሠረታዊ የመንግሥት አገልግሎት መስጫ ተቋማት በሙሉና በከፊል በመዘጋታቸው መቸገራቸውን ነዋሪዎች አመለከቱ። ከባለፈው ዓመት ጀምሮ የጤና፣ የትምህርት እና የግብር አሰባሰብ ሥራዎች በየአካባቢያቸው እየተከናወኑ አለመሆናቸውንም ለዶቼ ቬለ ተናግረዋል።\",\n",
        "  \"የሕዝብ ተወካዮች ምክር ቤት አባል እና የቋሚ ኮሚቴ ሰብሳቢ የነበሩት አቶ ክርስቲያን ታደለ እና የአማራ ክልል ምክር ቤት አባል የሆኑት አቶ ዮሐንስ ቧያለው ከቃሊቲ ወደ ቂሊንጦ ማረሚያ ቤት መዛወራቸውን ጠበቃቸው ተናገሩ።\",\n",
        "  \"ከተደጋጋሚ መሬት መንቀጥቀጥ በኋላ አፋር ክልል እሳት ከመሬት ውስጥ ሲፈላ ታይቷል፡፡ ከመሬት ውስጥ እሳትና ጭስ የሚተፋው እንፋሎቱ ዛሬ ማለዳውን 11 ሰዓት ግድም ከከባድ ፍንዳታ በኋላየተስተዋለ መሆኑን የአከባቢው ነዋሪዎች እና ባለስልጣናት ለዶቼ ቬለ ተናግረዋል፡፡ አለት የሚያፈናጥር እሳት ነው የተባለው እንፋሎቱ በክልሉ ጋቢረሱ (ዞን 03) ዱለቻ ወረዳ ሰጋንቶ ቀበሌ መከሰቱን የገለጹት የአከባቢው የአይን እማኞች ከዋናው ፍንዳታ በተጨማሪ በዙሪያው ተጨማሪ ፍንዳታዎች መታየት ቀጥሏል ባይ ናቸው፡፡\"\n",
        "]\n",
        "\n",
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings.shape)\n",
        "# [3, 384]\n",
        "\n",
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)"
      ],
      "metadata": {
        "id": "Wt8j2nlO4Hyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5ea3b1-8785-4289-cccc-99d0663351a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 512)\n",
            "tensor([[ 1.0000,  0.1440,  0.1889, -0.0929,  0.7447],\n",
            "        [ 0.1440,  1.0000,  0.1982,  0.2850,  0.2111],\n",
            "        [ 0.1889,  0.1982,  1.0000,  0.1963,  0.3434],\n",
            "        [-0.0929,  0.2850,  0.1963,  1.0000,  0.0580],\n",
            "        [ 0.7447,  0.2111,  0.3434,  0.0580,  1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyjAhXbLqDOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}